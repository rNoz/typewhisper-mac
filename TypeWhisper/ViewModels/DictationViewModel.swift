import AppKit
import Foundation
import Combine
import os

private let logger = Logger(subsystem: Bundle.main.bundleIdentifier ?? "typewhisper-mac", category: "DictationViewModel")

/// Orchestrates the dictation flow: recording → transcription → text insertion.
@MainActor
final class DictationViewModel: ObservableObject {
    nonisolated(unsafe) static var _shared: DictationViewModel?
    static var shared: DictationViewModel {
        guard let instance = _shared else {
            fatalError("DictationViewModel not initialized")
        }
        return instance
    }

    enum State: Equatable {
        case idle
        case recording
        case processing
        case inserting
        case promptSelection(String)    // text ready, user picks a prompt
        case promptProcessing(String)   // prompt name, LLM running
        case error(String)
    }

    @Published var state: State = .idle
    @Published var audioLevel: Float = 0
    @Published var recordingDuration: TimeInterval = 0
    @Published var hotkeyMode: HotkeyService.HotkeyMode?
    @Published var partialText: String = ""
    @Published var isStreaming: Bool = false
    @Published var whisperModeEnabled: Bool {
        didSet { UserDefaults.standard.set(whisperModeEnabled, forKey: UserDefaultsKeys.whisperModeEnabled) }
    }
    @Published var audioDuckingEnabled: Bool {
        didSet { UserDefaults.standard.set(audioDuckingEnabled, forKey: UserDefaultsKeys.audioDuckingEnabled) }
    }
    @Published var audioDuckingLevel: Double {
        didSet { UserDefaults.standard.set(audioDuckingLevel, forKey: UserDefaultsKeys.audioDuckingLevel) }
    }
    @Published var mediaPauseEnabled: Bool {
        didSet { UserDefaults.standard.set(mediaPauseEnabled, forKey: UserDefaultsKeys.mediaPauseEnabled) }
    }
    @Published var soundFeedbackEnabled: Bool {
        didSet { UserDefaults.standard.set(soundFeedbackEnabled, forKey: UserDefaultsKeys.soundFeedbackEnabled) }
    }
    @Published var singleKeyMode: Bool {
        didSet { UserDefaults.standard.set(singleKeyMode, forKey: UserDefaultsKeys.hotkeyUseSingleKey) }
    }
    @Published var singleKeyLabel: String
    @Published var activeProfileName: String?
    @Published var availablePromptActions: [PromptAction] = []
    @Published var selectedPromptIndex: Int = 0
    @Published var promptResultText: String = ""

    enum OverlayPosition: String, CaseIterable {
        case top
        case bottom
    }

    enum NotchIndicatorVisibility: String, CaseIterable {
        case always
        case duringActivity
        case never
    }

    enum NotchIndicatorContent: String, CaseIterable {
        case indicator
        case timer
        case waveform
        case clock
        case battery
        case none
    }

    @Published var overlayPosition: OverlayPosition {
        didSet { UserDefaults.standard.set(overlayPosition.rawValue, forKey: UserDefaultsKeys.overlayPosition) }
    }

    @Published var notchIndicatorVisibility: NotchIndicatorVisibility {
        didSet { UserDefaults.standard.set(notchIndicatorVisibility.rawValue, forKey: UserDefaultsKeys.notchIndicatorVisibility) }
    }

    @Published var notchIndicatorLeftContent: NotchIndicatorContent {
        didSet { UserDefaults.standard.set(notchIndicatorLeftContent.rawValue, forKey: UserDefaultsKeys.notchIndicatorLeftContent) }
    }

    @Published var notchIndicatorRightContent: NotchIndicatorContent {
        didSet { UserDefaults.standard.set(notchIndicatorRightContent.rawValue, forKey: UserDefaultsKeys.notchIndicatorRightContent) }
    }

    private let audioRecordingService: AudioRecordingService
    private let textInsertionService: TextInsertionService
    private let hotkeyService: HotkeyService
    private let modelManager: ModelManagerService
    private let settingsViewModel: SettingsViewModel
    private let historyService: HistoryService
    private let profileService: ProfileService
    private let translationService: TranslationService
    private let audioDuckingService: AudioDuckingService
    private let mediaPlaybackService: MediaPlaybackService
    private let dictionaryService: DictionaryService
    private let snippetService: SnippetService
    private let soundService: SoundService
    private let audioDeviceService: AudioDeviceService
    private let promptActionService: PromptActionService
    private let promptProcessingService: PromptProcessingService
    private var matchedProfile: Profile?
    private var capturedActiveApp: (name: String?, bundleId: String?, url: String?)?

    private var cancellables = Set<AnyCancellable>()
    private var recordingTimer: Timer?
    private var recordingStartTime: Date?
    private var streamingTask: Task<Void, Never>?
    private var transcriptionTask: Task<Void, Never>?
    private var silenceCancellable: AnyCancellable?
    private var errorResetTask: Task<Void, Never>?
    private var urlResolutionTask: Task<Void, Never>?
    private var promptDismissTask: Task<Void, Never>?

    init(
        audioRecordingService: AudioRecordingService,
        textInsertionService: TextInsertionService,
        hotkeyService: HotkeyService,
        modelManager: ModelManagerService,
        settingsViewModel: SettingsViewModel,
        historyService: HistoryService,
        profileService: ProfileService,
        translationService: TranslationService,
        audioDuckingService: AudioDuckingService,
        mediaPlaybackService: MediaPlaybackService,
        dictionaryService: DictionaryService,
        snippetService: SnippetService,
        soundService: SoundService,
        audioDeviceService: AudioDeviceService,
        promptActionService: PromptActionService,
        promptProcessingService: PromptProcessingService
    ) {
        self.audioRecordingService = audioRecordingService
        self.textInsertionService = textInsertionService
        self.hotkeyService = hotkeyService
        self.modelManager = modelManager
        self.settingsViewModel = settingsViewModel
        self.historyService = historyService
        self.profileService = profileService
        self.translationService = translationService
        self.audioDuckingService = audioDuckingService
        self.mediaPlaybackService = mediaPlaybackService
        self.dictionaryService = dictionaryService
        self.snippetService = snippetService
        self.soundService = soundService
        self.audioDeviceService = audioDeviceService
        self.promptActionService = promptActionService
        self.promptProcessingService = promptProcessingService
        self.whisperModeEnabled = UserDefaults.standard.bool(forKey: UserDefaultsKeys.whisperModeEnabled)
        self.audioDuckingEnabled = UserDefaults.standard.bool(forKey: UserDefaultsKeys.audioDuckingEnabled)
        self.audioDuckingLevel = UserDefaults.standard.object(forKey: UserDefaultsKeys.audioDuckingLevel) as? Double ?? 0.2
        self.mediaPauseEnabled = UserDefaults.standard.bool(forKey: UserDefaultsKeys.mediaPauseEnabled)
        self.soundFeedbackEnabled = UserDefaults.standard.object(forKey: UserDefaultsKeys.soundFeedbackEnabled) as? Bool ?? true
        let useSingleKey = UserDefaults.standard.bool(forKey: UserDefaultsKeys.hotkeyUseSingleKey)
        self.singleKeyMode = useSingleKey
        let isFn = UserDefaults.standard.bool(forKey: UserDefaultsKeys.singleKeyIsFn)
        let code = UInt16(UserDefaults.standard.integer(forKey: UserDefaultsKeys.singleKeyCode))
        self.singleKeyLabel = useSingleKey
            ? HotkeyService.keyName(for: code, isFn: isFn)
            : ""
        self.overlayPosition = UserDefaults.standard.string(forKey: UserDefaultsKeys.overlayPosition)
            .flatMap { OverlayPosition(rawValue: $0) } ?? .top
        self.notchIndicatorVisibility = UserDefaults.standard.string(forKey: UserDefaultsKeys.notchIndicatorVisibility)
            .flatMap { NotchIndicatorVisibility(rawValue: $0) } ?? .duringActivity
        self.notchIndicatorLeftContent = UserDefaults.standard.string(forKey: UserDefaultsKeys.notchIndicatorLeftContent)
            .flatMap { NotchIndicatorContent(rawValue: $0) } ?? .timer
        self.notchIndicatorRightContent = UserDefaults.standard.string(forKey: UserDefaultsKeys.notchIndicatorRightContent)
            .flatMap { NotchIndicatorContent(rawValue: $0) } ?? .waveform

        setupBindings()
    }

    var canDictate: Bool {
        modelManager.activeEngine?.isModelLoaded == true
    }

    var needsMicPermission: Bool {
        !audioRecordingService.hasMicrophonePermission
    }

    var needsAccessibilityPermission: Bool {
        !textInsertionService.isAccessibilityGranted
    }

    private func setupBindings() {
        hotkeyService.onDictationStart = { [weak self] in
            self?.startRecording()
        }

        hotkeyService.onDictationStop = { [weak self] in
            self?.stopDictation()
        }

        audioRecordingService.$audioLevel
            .dropFirst()
            .sink { [weak self] level in
                DispatchQueue.main.async {
                    self?.audioLevel = level
                }
            }
            .store(in: &cancellables)

        hotkeyService.$currentMode
            .dropFirst()
            .sink { [weak self] mode in
                DispatchQueue.main.async {
                    self?.hotkeyMode = mode
                }
            }
            .store(in: &cancellables)

        audioDeviceService.$disconnectedDeviceName
            .compactMap { $0 }
            .sink { [weak self] _ in
                guard let self, self.state == .recording else { return }
                self.stopDictation()
                self.hotkeyService.cancelDictation()
                self.showError(String(localized: "Microphone disconnected. Falling back to system default."))
            }
            .store(in: &cancellables)
    }

    private func startRecording() {
        guard canDictate else {
            showError("No model loaded. Please download a model first.")
            return
        }

        guard audioRecordingService.hasMicrophonePermission else {
            showError("Microphone permission required.")
            return
        }

        // Cancel any pending transcription from a previous recording
        transcriptionTask?.cancel()
        transcriptionTask = nil

        // Match profile based on active app — store for reuse in stopDictation
        let activeApp = textInsertionService.captureActiveApp()
        capturedActiveApp = activeApp
        matchedProfile = profileService.matchProfile(bundleIdentifier: activeApp.bundleId, url: nil)
        activeProfileName = matchedProfile?.name

        // Apply gain boost: profile override ?? global setting
        let effectiveWhisperMode = matchedProfile?.whisperModeOverride ?? whisperModeEnabled
        audioRecordingService.gainMultiplier = effectiveWhisperMode ? 4.0 : 1.0

        // Resolve browser URL asynchronously to avoid blocking the main thread.
        // If a more specific URL profile matches, update the active profile on the fly.
        if let bundleId = activeApp.bundleId {
            urlResolutionTask = Task { [weak self] in
                guard let self else { return }
                logger.info("URL resolution: starting for bundleId=\(bundleId)")
                let resolvedURL = await textInsertionService.resolveBrowserURL(bundleId: bundleId)
                logger.info("URL resolution: resolvedURL=\(resolvedURL ?? "nil"), state=\(String(describing: self.state))")
                guard state == .recording || state == .processing else {
                    logger.info("URL resolution: skipped - state is \(String(describing: self.state))")
                    return
                }
                guard let currentApp = capturedActiveApp, currentApp.bundleId == bundleId else {
                    logger.info("URL resolution: skipped - bundleId mismatch")
                    return
                }

                capturedActiveApp = (name: currentApp.name, bundleId: currentApp.bundleId, url: resolvedURL)

                guard let resolvedURL else {
                    logger.info("URL resolution: no URL resolved")
                    return
                }
                guard let refinedProfile = profileService.matchProfile(bundleIdentifier: bundleId, url: resolvedURL) else {
                    logger.info("URL resolution: no profile matched for URL \(resolvedURL)")
                    return
                }

                logger.info("URL resolution: matched profile '\(refinedProfile.name)'")
                matchedProfile = refinedProfile
                activeProfileName = refinedProfile.name
                let refinedWhisperMode = refinedProfile.whisperModeOverride ?? whisperModeEnabled
                audioRecordingService.gainMultiplier = refinedWhisperMode ? 4.0 : 1.0
            }
        }

        do {
            audioRecordingService.selectedDeviceID = audioDeviceService.selectedDeviceID
            try audioRecordingService.startRecording()
            if audioDuckingEnabled {
                audioDuckingService.duckAudio(to: Float(audioDuckingLevel))
            }
            if mediaPauseEnabled {
                mediaPlaybackService.pausePlayback()
            }
            state = .recording
            soundService.play(.recordingStarted, enabled: soundFeedbackEnabled)
            partialText = ""
            recordingStartTime = Date()
            startRecordingTimer()
            startStreamingIfSupported()
            startSilenceDetection()
        } catch {
            audioDuckingService.restoreAudio()
            mediaPlaybackService.resumePlayback()
            soundService.play(.error, enabled: soundFeedbackEnabled)
            showError(error.localizedDescription)
            hotkeyService.cancelDictation()
        }
    }

    private var effectiveLanguage: String? {
        if let profileLang = matchedProfile?.inputLanguage {
            return profileLang == "auto" ? nil : profileLang
        }
        return settingsViewModel.selectedLanguage
    }

    private var effectiveTask: TranscriptionTask {
        if let profileTask = matchedProfile?.selectedTask,
           let task = TranscriptionTask(rawValue: profileTask) {
            return task
        }
        return settingsViewModel.selectedTask
    }

    private var effectiveTranslationTarget: String? {
        if let profileTarget = matchedProfile?.translationTargetLanguage {
            return profileTarget
        }
        if settingsViewModel.translationEnabled {
            return settingsViewModel.translationTargetLanguage
        }
        return nil
    }

    private var effectiveEngineOverride: EngineType? {
        guard let raw = matchedProfile?.engineOverride else { return nil }
        return EngineType(rawValue: raw)
    }

    private var effectiveCloudModelOverride: String? {
        matchedProfile?.cloudModelOverride
    }

    private var effectivePromptAction: PromptAction? {
        guard let actionId = matchedProfile?.promptActionId else { return nil }
        return promptActionService.action(byId: actionId)
    }

    private func stopDictation() {
        guard state == .recording else { return }

        audioDuckingService.restoreAudio()
        mediaPlaybackService.resumePlayback()
        stopStreaming()
        stopSilenceDetection()
        stopRecordingTimer()
        let samples = audioRecordingService.stopRecording()

        guard !samples.isEmpty else {
            resetDictationState()
            return
        }

        let audioDuration = Double(samples.count) / 16000.0
        guard audioDuration >= 0.3 else {
            // Too short to transcribe meaningfully
            resetDictationState()
            return
        }

        state = .processing

        transcriptionTask = Task {
            do {
                // Wait for browser URL resolution so URL-based profile overrides apply
                await urlResolutionTask?.value

                let activeApp = capturedActiveApp ?? textInsertionService.captureActiveApp()
                let language = effectiveLanguage
                let task = effectiveTask
                let engineOverride = effectiveEngineOverride
                let cloudModelOverride = effectiveCloudModelOverride
                let translationTarget = effectiveTranslationTarget
                let termsPrompt = dictionaryService.getTermsForPrompt()

                let result = try await modelManager.transcribe(
                    audioSamples: samples,
                    language: language,
                    task: task,
                    engineOverride: engineOverride,
                    cloudModelOverride: cloudModelOverride,
                    prompt: termsPrompt
                )

                // Bail out if a new recording started while we were transcribing
                guard !Task.isCancelled else { return }

                var text = result.text.trimmingCharacters(in: .whitespacesAndNewlines)
                guard !text.isEmpty else {
                    resetDictationState()
                    return
                }

                // Prompt processing replaces translation when active
                if let promptAction = self.effectivePromptAction {
                    text = try await promptProcessingService.process(
                        prompt: promptAction.prompt,
                        text: text,
                        providerOverride: promptAction.providerType.flatMap { LLMProviderType(rawValue: $0) },
                        cloudModelOverride: promptAction.cloudModel
                    )
                } else if let targetCode = translationTarget {
                    let target = Locale.Language(identifier: targetCode)
                    text = try await translationService.translate(text: text, to: target)
                }

                guard !Task.isCancelled else { return }

                // Post-processing pipeline
                text = snippetService.applySnippets(to: text)
                text = dictionaryService.applyCorrections(to: text)

                partialText = ""
                _ = try await textInsertionService.insertText(text)

                let modelDisplayName = modelManager.resolvedModelDisplayName(
                    engineOverride: engineOverride,
                    cloudModelOverride: cloudModelOverride
                )

                historyService.addRecord(
                    rawText: result.text,
                    finalText: text,
                    appName: activeApp.name,
                    appBundleIdentifier: activeApp.bundleId,
                    appURL: activeApp.url,
                    durationSeconds: audioDuration,
                    language: language,
                    engineUsed: result.engineUsed.rawValue,
                    modelUsed: modelDisplayName
                )

                soundService.play(.transcriptionSuccess, enabled: soundFeedbackEnabled)

                state = .inserting

                try? await Task.sleep(for: .seconds(1.5))
                guard !Task.isCancelled else { return }
                resetDictationState()
            } catch {
                guard !Task.isCancelled else { return }
                soundService.play(.error, enabled: soundFeedbackEnabled)
                showError(error.localizedDescription)
                matchedProfile = nil
                capturedActiveApp = nil
                activeProfileName = nil
            }
        }
    }

    func requestMicPermission() {
        Task {
            _ = await audioRecordingService.requestMicrophonePermission()
            DispatchQueue.main.async { [weak self] in
                self?.objectWillChange.send()
            }
            pollPermissionStatus()
        }
    }

    func requestAccessibilityPermission() {
        textInsertionService.requestAccessibilityPermission()
        pollPermissionStatus()
    }

    func setSingleKey(code: UInt16, isFn: Bool) {
        let label = HotkeyService.keyName(for: code, isFn: isFn)
        singleKeyLabel = label
        singleKeyMode = true
        hotkeyService.updateSingleKey(code: code, isFn: isFn)
    }

    func disableSingleKey() {
        singleKeyMode = false
        singleKeyLabel = ""
        hotkeyService.disableSingleKey()
    }

    private var permissionPollTask: Task<Void, Never>?

    /// Polls permission status periodically until granted or timeout.
    private func pollPermissionStatus() {
        permissionPollTask?.cancel()
        permissionPollTask = Task {
            for _ in 0..<30 {
                try? await Task.sleep(for: .seconds(1))
                guard !Task.isCancelled else { return }
                DispatchQueue.main.async { [weak self] in
                    self?.objectWillChange.send()
                }
                if !needsMicPermission, !needsAccessibilityPermission { return }
            }
        }
    }

    private func resetDictationState() {
        promptDismissTask?.cancel()
        errorResetTask?.cancel()
        urlResolutionTask?.cancel()
        urlResolutionTask = nil
        state = .idle
        partialText = ""
        matchedProfile = nil
        capturedActiveApp = nil
        activeProfileName = nil
    }

    // MARK: - Prompt Selection

    func enterPromptSelection(with text: String) {
        let actions = promptActionService.getEnabledActions()
        guard !actions.isEmpty else {
            // No prompts configured, stay in clipboard-only mode
            state = .inserting
            Task {
                try? await Task.sleep(for: .seconds(1.5))
                guard !Task.isCancelled else { return }
                resetDictationState()
            }
            return
        }
        availablePromptActions = actions
        selectedPromptIndex = 0
        state = .promptSelection(text)

        promptDismissTask?.cancel()
        promptDismissTask = Task {
            try? await Task.sleep(for: .seconds(30))
            guard !Task.isCancelled else { return }
            if case .promptSelection = state {
                dismissPromptSelection()
            }
        }
    }

    func selectPromptAction(_ action: PromptAction) {
        guard case .promptSelection(let text) = state else { return }

        promptDismissTask?.cancel()
        state = .promptProcessing(action.name)
        promptResultText = ""

        transcriptionTask = Task {
            do {
                let result = try await promptProcessingService.process(
                    prompt: action.prompt,
                    text: text,
                    providerOverride: action.providerType.flatMap { LLMProviderType(rawValue: $0) },
                    cloudModelOverride: action.cloudModel
                )
                guard !Task.isCancelled else { return }

                promptResultText = result

                // Copy result to clipboard
                let pasteboard = NSPasteboard.general
                pasteboard.clearContents()
                pasteboard.setString(result, forType: .string)

                soundService.play(.transcriptionSuccess, enabled: soundFeedbackEnabled)
                state = .inserting

                try? await Task.sleep(for: .seconds(2))
                guard !Task.isCancelled else { return }
                resetDictationState()
            } catch {
                guard !Task.isCancelled else { return }
                soundService.play(.error, enabled: soundFeedbackEnabled)
                showError(error.localizedDescription)
            }
        }
    }

    func selectPromptByIndex(_ index: Int) {
        guard index >= 0, index < availablePromptActions.count else { return }
        selectPromptAction(availablePromptActions[index])
    }

    func movePromptSelection(by offset: Int) {
        guard !availablePromptActions.isEmpty else { return }
        selectedPromptIndex = max(0, min(availablePromptActions.count - 1, selectedPromptIndex + offset))
    }

    func confirmPromptSelection() {
        guard selectedPromptIndex >= 0, selectedPromptIndex < availablePromptActions.count else { return }
        selectPromptAction(availablePromptActions[selectedPromptIndex])
    }

    func dismissPromptSelection() {
        resetDictationState()
    }

    private func showError(_ message: String) {
        state = .error(message)
        errorResetTask?.cancel()
        errorResetTask = Task {
            try? await Task.sleep(for: .seconds(3))
            if case .error = state {
                state = .idle
            }
        }
    }

    // MARK: - Streaming

    /// Text confirmed from previous streaming passes — never changes once set.
    private var confirmedStreamingText = ""

    private func startStreamingIfSupported() {
        let resolvedEngine = modelManager.resolveEngine(override: effectiveEngineOverride, cloudModelOverride: effectiveCloudModelOverride)
        guard let engine = resolvedEngine, engine.supportsStreaming else { return }

        isStreaming = true
        confirmedStreamingText = ""
        let streamLanguage = effectiveLanguage
        let streamTask = effectiveTask
        let streamEngineOverride = effectiveEngineOverride
        let streamCloudModelOverride = effectiveCloudModelOverride
        let streamPrompt = dictionaryService.getTermsForPrompt()
        streamingTask = Task { [weak self] in
            guard let self else { return }
            // Initial delay before first streaming attempt
            try? await Task.sleep(for: .seconds(1.5))

            while !Task.isCancelled, self.state == .recording {
                let buffer = self.audioRecordingService.getRecentBuffer(maxDuration: 3600)
                let bufferDuration = Double(buffer.count) / 16000.0

                if bufferDuration > 0.5 {
                    do {
                        let confirmed = self.confirmedStreamingText
                        let result = try await self.modelManager.transcribe(
                            audioSamples: buffer,
                            language: streamLanguage,
                            task: streamTask,
                            engineOverride: streamEngineOverride,
                            cloudModelOverride: streamCloudModelOverride,
                            prompt: streamPrompt,
                            onProgress: { [weak self] text in
                                guard let self, !Task.isCancelled else { return false }
                                let stable = Self.stabilizeText(confirmed: confirmed, new: text)
                                DispatchQueue.main.async {
                                    if self.partialText != stable {
                                        self.partialText = stable
                                    }
                                }
                                return true
                            }
                        )
                        let text = result.text.trimmingCharacters(in: .whitespacesAndNewlines)
                        if !text.isEmpty {
                            let stable = Self.stabilizeText(confirmed: confirmed, new: text)
                            if self.partialText != stable {
                                self.partialText = stable
                            }
                            self.confirmedStreamingText = stable
                        }
                    } catch {
                        // Streaming errors are non-fatal; final transcription will still run
                    }
                }

                try? await Task.sleep(for: .seconds(1.5))
            }
        }
    }

    private func stopStreaming() {
        streamingTask?.cancel()
        streamingTask = nil
        isStreaming = false
        confirmedStreamingText = ""
    }

    /// Keeps confirmed text stable and only appends new content.
    nonisolated private static func stabilizeText(confirmed: String, new: String) -> String {
        let new = new.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !confirmed.isEmpty else { return new }
        guard !new.isEmpty else { return confirmed }

        // Best case: new text starts with confirmed text
        if new.hasPrefix(confirmed) { return new }

        // Find how far the texts match from the start
        let confirmedChars = Array(confirmed.unicodeScalars)
        let newChars = Array(new.unicodeScalars)
        var matchEnd = 0
        for i in 0..<min(confirmedChars.count, newChars.count) {
            if confirmedChars[i] == newChars[i] {
                matchEnd = i + 1
            } else {
                break
            }
        }

        // If more than half matches, keep confirmed and append the new tail
        if matchEnd > confirmed.count / 2 {
            let newContent = String(new.unicodeScalars.dropFirst(matchEnd))
            return confirmed + newContent
        }

        // Suffix-prefix overlap: new text starts with a suffix of confirmed
        // (happens when the streaming window has shifted forward)
        let minOverlap = min(20, confirmedChars.count / 4)
        let maxShift = min(confirmedChars.count - minOverlap, 150)
        if maxShift > 0 {
            for dropCount in 1...maxShift {
                let suffix = String(confirmed.unicodeScalars.dropFirst(dropCount))
                if new.hasPrefix(suffix) {
                    let newTail = String(new.unicodeScalars.dropFirst(confirmed.unicodeScalars.count - dropCount))
                    return newTail.isEmpty ? confirmed : confirmed + newTail
                }
            }
        }

        // Very different result — accept the new text to avoid freezing the preview
        return new
    }

    // MARK: - Silence Detection

    private func startSilenceDetection() {
        // Only auto-stop in toggle mode, not push-to-talk
        guard hotkeyMode == .toggle else { return }

        silenceCancellable = audioRecordingService.$silenceDuration
            .dropFirst()
            .sink { [weak self] duration in
                DispatchQueue.main.async {
                    guard let self, self.state == .recording else { return }
                    if duration >= self.audioRecordingService.silenceAutoStopDuration {
                        self.audioRecordingService.didAutoStop = true
                        self.stopDictation()
                        self.hotkeyService.cancelDictation()
                    }
                }
            }
    }

    private func stopSilenceDetection() {
        silenceCancellable?.cancel()
        silenceCancellable = nil
    }

    private func startRecordingTimer() {
        recordingDuration = 0
        recordingTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] _ in
            Task { @MainActor [weak self] in
                guard let self, let start = self.recordingStartTime else { return }
                self.recordingDuration = Date().timeIntervalSince(start)
            }
        }
    }

    private func stopRecordingTimer() {
        recordingTimer?.invalidate()
        recordingTimer = nil
        recordingDuration = 0
    }
}
